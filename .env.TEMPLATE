# ============================================================================
# Environment Configuration Template - Whisper Project
# ============================================================================
# 
# INSTRUCTIONS:
# 1. Copy this file to `.env` in your working directory
# 2. Fill in your actual API keys and configuration values
# 3. Never commit `.env` to version control
# 4. For CI/deployment, use environment variables or a secure vault
#
# DOCUMENTATION:
# - https://platform.openai.com/docs/api-reference/audio/create-transcription
# - https://www.assemblyai.com/docs
# - https://www.rev.com/api/documentation
#

# ============================================================================
# TRANSCRIPTION API CREDENTIALS (Choose one or more)
# ============================================================================

# OpenAI API Configuration
# Required for OpenAI Whisper API transcription
# Get your key at: https://platform.openai.com/api-keys
# Format: sk-... (keep this secret!)
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Override API base URL for proxies or custom endpoints
OPENAI_BASE_URL=https://api.openai.com/v1

# AssemblyAI Configuration
# Alternative transcription service
# Get your key at: https://www.assemblyai.com/dashboard
ASSEMBLYAI_API_KEY=your_assemblyai_api_key_here

# Rev.com AI Configuration
# Another alternative transcription service
# Get your key at: https://www.rev.com/api
REV_API_KEY=your_rev_api_key_here

# Speechmatics Configuration
# Yet another transcription option
# Get your key at: https://console.speechmatics.com
SPEECHMATICS_API_KEY=your_speechmatics_api_key_here

# ============================================================================
# MODEL & PROCESSING CONFIGURATION
# ============================================================================

# Default whisper model to use if not specified per request
# Options: tiny, base, small, medium, large
# Larger models are more accurate but slower and consume more memory
DEFAULT_MODEL=medium

# Default language for transcription (ISO 639-1 code)
# Examples: en (English), es (Spanish), fr (French), auto (auto-detect)
DEFAULT_LANGUAGE=auto

# Output directory for transcription results
# Can be relative (./output) or absolute (/home/user/transcripts)
OUTPUT_DIR=./output

# ============================================================================
# DOCKER & GPU CONFIGURATION (Optional, for containerized deployment)
# ============================================================================

# NVIDIA GPU device visibility (for GPU acceleration in Docker)
# Leave commented if running on CPU or without NVIDIA Docker support
# NVIDIA_VISIBLE_DEVICES=all

# Whisper model cache directory (for persistent model storage)
# Useful in Docker to avoid re-downloading models on each run
# WHISPER_CACHE_DIR=/path/to/models
